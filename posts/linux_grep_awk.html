<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- canonical URL -->
    <link rel="canonical" href="https://braindetox.kr/blog.html?post=linux_grep_awk">
    <title>리눅스 Grep과 AWK 사용 가이드 - BrainDetox 기술 블로그</title>
    <meta name="description" content="리눅스 Grep과 AWK 사용 가이드 - 리눅스 카테고리의 기술 블로그 글입니다.">
    <meta name="keywords" content="리눅스, grep, awk, 텍스트 처리, 명령어, 정규표현식, Linux, command line, text processing, regular expression">
    
    <!-- Open Graph / 소셜 미디어 -->
    <meta property="og:title" content="리눅스 Grep과 AWK 사용 가이드 - BrainDetox 기술 블로그">
    <meta property="og:description" content="리눅스 Grep과 AWK 사용 가이드 - 리눅스 카테고리의 기술 블로그 글입니다.">
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://braindetox.kr/blog.html?post=linux_grep_awk">
    <meta property="og:image" content="https://braindetox.kr/site_logo.png">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">
    <meta property="article:published_time" content="2025-06-02">
    <meta property="article:section" content="리눅스">
    
    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="리눅스 Grep과 AWK 사용 가이드 - BrainDetox 기술 블로그">
    <meta name="twitter:description" content="리눅스 Grep과 AWK 사용 가이드 - 리눅스 카테고리의 기술 블로그 글입니다.">
    <meta name="twitter:image" content="https://braindetox.kr/site_logo.png">
    
    <!-- 구조화된 데이터 - JSON-LD -->
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "리눅스 Grep과 AWK 사용 가이드",
      "alternativeHeadline": "Linux Grep and AWK Usage Guide",
      "datePublished": "2025-06-02",
      "dateModified": "2025-06-02",
      "author": {
        "@type": "Person",
        "name": "BrainDetox"
      },
      "keywords": "리눅스, grep, awk, 텍스트 처리, 명령어, 정규표현식, Linux, command line, text processing, regular expression",
      "publisher": {
        "@type": "Organization",
        "name": "BrainDetox",
        "logo": {
          "@type": "ImageObject",
          "url": "https://braindetox.kr/site_logo.png"
        }
      },
      "image": {
        "@type": "ImageObject",
        "url": "https://braindetox.kr/site_logo.png"
      }
      "mainEntityOfPage": "https://braindetox.kr/blog.html?post=linux_grep_awk"
  "mainEntityOfPage": "https://braindetox.kr/blog.html?post=linux_grep_awk"
}
    </script>
</head>
<body>
    <article itemscope itemtype="https://schema.org/BlogPosting">
        <meta itemprop="headline" content="리눅스 Grep과 AWK 사용 가이드">
        <meta itemprop="datePublished" content="2025-06-02">
        <meta itemprop="dateModified" content="2025-06-02">
        <meta itemprop="author" content="BrainDetox">
        <meta itemprop="keywords" content="리눅스, grep, awk, 텍스트 처리, 명령어, 정규표현식, Linux, command line, text processing, regular expression">
        <meta itemprop="image" content="https://braindetox.kr/site_logo.png">
        
        <div class="post-content-header">
            <h1 class="post-title" itemprop="name">리눅스 Grep과 AWK 사용 가이드</h1>
            <h2 class="post-subtitle">Linux Grep and AWK Usage Guide</h2>
            <div class="post-meta">
                <span class="post-date" itemprop="datePublished" content="2025-06-02">2025-06-02</span>
                <span class="post-category" itemprop="articleSection">리눅스</span>
            </div>
        </div>
        <div class="post-content" itemprop="articleBody">
            <h1>리눅스 텍스트 처리 심층 분석: Grep과 AWK 마스터하기</h1>
<p>리눅스 및 유닉스 계열 시스템에서 텍스트 데이터는 운영의 핵심입니다. 로그 파일, 설정 파일, 명령어 출력 등 모든 것이 텍스트 형태로 존재하며, 이를 효과적으로 다루는 능력은 시스템 관리자, 개발자, 데이터 분석가 모두에게 필수적입니다. 이 글에서는 리눅스에서 가장 강력하고 널리 사용되는 텍스트 처리 도구인 grep과 awk의 기본 사용법부터 고급 활용법까지 상세하게 알아봅니다. 이 두 도구를 능숙하게 사용하면 복잡한 텍스트 처리 작업을 자동화하고, 방대한 데이터에서 원하는 정보를 신속하게 추출하며, 시스템 모니터링 및 분석 작업을 효율적으로 수행할 수 있습니다.</p>
<h2>Grep (Global Regular Expression Print) 명령어 심층 분석</h2>
<p><code>grep</code>은 파일이나 표준 입력으로부터 특정 패턴(주로 정규표현식)과 일치하는 라인을 찾아 출력하는 강력한 명령줄 유틸리티입니다. 단순한 문자열 검색부터 복잡한 패턴 매칭까지 다양한 기능을 제공합니다.</p>
<h3>기본 사용법</h3>
<pre><code class="language-bash">grep [옵션] 패턴 [파일...]
</code></pre>
<ul>
<li><code>[옵션]</code>: 검색 방식을 제어하는 다양한 옵션들입니다.</li>
<li><code>패턴</code>: 검색할 문자열 또는 정규표현식입니다.</li>
<li><code>[파일...]</code>: 검색 대상 파일입니다. 생략 시 표준 입력(stdin)을 사용합니다.</li>
</ul>
<h3>자주 사용하고 유용한 옵션 상세</h3>
<table>
<thead>
<tr>
<th align="left">옵션</th>
<th align="left">설명</th>
<th align="left">예시</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>-i</code> (<code>--ignore-case</code>)</td>
<td align="left">대소문자를 구분하지 않고 검색합니다.</td>
<td align="left"><code>grep -i &quot;error&quot; log.txt</code></td>
</tr>
<tr>
<td align="left"><code>-n</code> (<code>--line-number</code>)</td>
<td align="left">일치하는 라인의 번호를 함께 출력합니다.</td>
<td align="left"><code>grep -n &quot;warning&quot; log.txt</code></td>
</tr>
<tr>
<td align="left"><code>-v</code> (<code>--invert-match</code>)</td>
<td align="left">패턴과 일치하지 <em>않는</em> 라인만 검색합니다.</td>
<td align="left"><code>grep -v &quot;success&quot; log.txt</code></td>
</tr>
<tr>
<td align="left"><code>-c</code> (<code>--count</code>)</td>
<td align="left">패턴과 일치하는 라인의 총 개수만 출력합니다.</td>
<td align="left"><code>grep -c &quot;debug&quot; app.log</code></td>
</tr>
<tr>
<td align="left"><code>-l</code> (<code>--files-with-matches</code>)</td>
<td align="left">패턴과 일치하는 라인이 포함된 파일의 이름만 출력합니다. (내용은 출력 안 함)</td>
<td align="left"><code>grep -l &quot;TODO&quot; *.java</code></td>
</tr>
<tr>
<td align="left"><code>-L</code> (<code>--files-without-match</code>)</td>
<td align="left">패턴과 일치하는 라인이 없는 파일의 이름만 출력합니다.</td>
<td align="left"><code>grep -L &quot;FINAL_VERSION&quot; *.txt</code></td>
</tr>
<tr>
<td align="left"><code>-r</code> 또는 <code>-R</code> (<code>--recursive</code>)</td>
<td align="left">지정된 디렉토리와 그 하위 디렉토리까지 재귀적으로 검색합니다.</td>
<td align="left"><code>grep -r &quot;config_value&quot; /etc/project/</code></td>
</tr>
<tr>
<td align="left"><code>-w</code> (<code>--word-regexp</code>)</td>
<td align="left">패턴이 독립된 단어(word)로 일치하는 경우만 검색합니다. (공백이나 특수문자로 구분)</td>
<td align="left"><code>grep -w &quot;main&quot; program.c</code> (main_function은 매칭, print_main은 불일치)</td>
</tr>
<tr>
<td align="left"><code>-x</code> (<code>--line-regexp</code>)</td>
<td align="left">패턴이 라인 전체와 정확히 일치하는 경우만 검색합니다.</td>
<td align="left"><code>grep -x &quot;localhost:8080&quot; access.log</code></td>
</tr>
<tr>
<td align="left"><code>-E</code> (<code>--extended-regexp</code>)</td>
<td align="left">확장 정규표현식(ERE)을 사용합니다. <code>egrep</code> 명령어와 동일합니다.</td>
<td align="left">`grep -E &quot;error</td>
</tr>
<tr>
<td align="left"><code>-F</code> (<code>--fixed-strings</code>)</td>
<td align="left">패턴을 정규표현식이 아닌 고정된 문자열로 취급합니다. (특수문자도 일반문자로)</td>
<td align="left"><code>grep -F &quot;*ERROR*&quot; log.txt</code> (*를 문자 그대로 검색)</td>
</tr>
<tr>
<td align="left"><code>-A NUM</code> (<code>--after-context=NUM</code>)</td>
<td align="left">일치하는 라인과 그 이후 <code>NUM</code>개의 라인을 함께 출력합니다.</td>
<td align="left"><code>grep -A 2 &quot;Exception&quot; error.log</code></td>
</tr>
<tr>
<td align="left"><code>-B NUM</code> (<code>--before-context=NUM</code>)</td>
<td align="left">일치하는 라인과 그 이전 <code>NUM</code>개의 라인을 함께 출력합니다.</td>
<td align="left"><code>grep -B 3 &quot;SEGFAULT&quot; core_dump.txt</code></td>
</tr>
<tr>
<td align="left"><code>-C NUM</code> (<code>--context=NUM</code>)</td>
<td align="left">일치하는 라인과 그 이전/이후 <code>NUM</code>개의 라인을 함께 출력합니다. (<code>-A NUM -B NUM</code>과 유사)</td>
<td align="left"><code>grep -C 1 &quot;critical error&quot; system.log</code></td>
</tr>
<tr>
<td align="left"><code>--color=auto</code></td>
<td align="left">검색된 패턴을 다른 색으로 강조하여 보여줍니다. (대부분 시스템에서 기본값)</td>
<td align="left"><code>grep --color=auto &quot;pattern&quot; file.txt</code></td>
</tr>
<tr>
<td align="left"><code>-o</code> (<code>--only-matching</code>)</td>
<td align="left">일치하는 라인 전체가 아닌, 패턴과 일치하는 부분만 출력합니다. 각 매칭은 새 줄에 표시.</td>
<td align="left"><code>grep -o -E &quot;[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}&quot; access.log</code></td>
</tr>
</tbody></table>
<h3>Grep과 정규표현식 (Regular Expressions)</h3>
<p><code>grep</code>의 진정한 강력함은 정규표현식과 함께 사용할 때 발휘됩니다. 정규표현식은 텍스트의 특정 패턴을 기술하는 형식 언어입니다.</p>
<ul>
<li><strong>기본 정규표현식 (BRE - Basic Regular Expressions)</strong>: <code>grep</code>의 기본 모드입니다. <code>*</code>, <code>.</code>, <code>^</code>, <code>$</code>, <code>[]</code>, <code>\</code> 등의 메타문자를 사용합니다. <code>+</code>, <code>?</code>, <code>|</code>, <code>()</code> 같은 일부 메타문자는 앞에 <code>\</code>를 붙여야 특별한 의미로 사용됩니다 (예: <code>\+</code>, <code>\?</code>, <code>\|</code>, <code>\(\)</code>).</li>
<li><strong>확장 정규표현식 (ERE - Extended Regular Expressions)</strong>: <code>grep -E</code> 또는 <code>egrep</code>을 사용합니다. BRE의 모든 기능에 더해 <code>+</code>, <code>?</code>, <code>|</code>, <code>()</code> 등을 <code>\</code> 없이 사용할 수 있어 표현이 간결해집니다.</li>
</ul>
<p><strong>주요 정규표현식 메타문자 및 구성 요소:</strong></p>
<table>
<thead>
<tr>
<th align="left">메타문자/구성요소</th>
<th align="left">설명</th>
<th align="left">BRE 예시 (필요시 <code>\</code>)</th>
<th align="left">ERE 예시 (<code>grep -E</code>)</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>.</code></td>
<td align="left">임의의 한 문자와 일치 (줄바꿈 문자 제외)</td>
<td align="left"><code>a.c</code></td>
<td align="left"><code>a.c</code></td>
</tr>
<tr>
<td align="left"><code>*</code></td>
<td align="left">바로 앞의 문자가 0번 이상 반복될 때 일치</td>
<td align="left"><code>ab*c</code> (ac, abc, abbc...)</td>
<td align="left"><code>ab*c</code></td>
</tr>
<tr>
<td align="left"><code>+</code></td>
<td align="left">바로 앞의 문자가 1번 이상 반복될 때 일치 (ERE)</td>
<td align="left"><code>ab\+c</code></td>
<td align="left"><code>ab+c</code> (abc, abbc...)</td>
</tr>
<tr>
<td align="left"><code>?</code></td>
<td align="left">바로 앞의 문자가 0번 또는 1번 나타날 때 일치 (ERE)</td>
<td align="left"><code>ab\?c</code></td>
<td align="left"><code>ab?c</code> (ac, abc)</td>
</tr>
<tr>
<td align="left"><code>^</code></td>
<td align="left">라인의 시작 부분과 일치</td>
<td align="left"><code>^start</code></td>
<td align="left"><code>^start</code></td>
</tr>
<tr>
<td align="left"><code>$</code></td>
<td align="left">라인의 끝 부분과 일치</td>
<td align="left"><code>end$</code></td>
<td align="left"><code>end$</code></td>
</tr>
<tr>
<td align="left"><code>[abc]</code></td>
<td align="left">대괄호 안의 문자 중 하나와 일치 (a 또는 b 또는 c)</td>
<td align="left"><code>[aeiou]</code></td>
<td align="left"><code>[aeiou]</code></td>
</tr>
<tr>
<td align="left"><code>[^abc]</code></td>
<td align="left">대괄호 안의 문자를 제외한 나머지 문자 중 하나와 일치</td>
<td align="left"><code>[^0-9]</code> (숫자 제외)</td>
<td align="left"><code>[^0-9]</code></td>
</tr>
<tr>
<td align="left"><code>[a-z]</code></td>
<td align="left">a부터 z까지의 문자 중 하나와 일치 (범위 지정)</td>
<td align="left"><code>[A-Za-z0-9]</code></td>
<td align="left"><code>[A-Za-z0-9]</code></td>
</tr>
<tr>
<td align="left"><code>|</code> (BRE) <code>|</code> (ERE)</td>
<td align="left">OR 조건. 앞 또는 뒤의 패턴 중 하나와 일치</td>
<td align="left"><code>apple|orange</code></td>
<td align="left">`apple</td>
</tr>
<tr>
<td align="left"><code>\(...\)</code> (BRE) <code>(...)</code> (ERE)</td>
<td align="left">그룹화. 패턴의 일부를 묶어 적용 범위를 지정하거나, 역참조를 위해 사용</td>
<td align="left"><code>\(ab\)\+</code></td>
<td align="left"><code>(ab)+</code></td>
</tr>
<tr>
<td align="left"><code>{n}</code></td>
<td align="left">바로 앞의 문자가 정확히 n번 반복 (ERE)</td>
<td align="left"><code>a\{3\}</code></td>
<td align="left"><code>a{3}</code> (aaa)</td>
</tr>
<tr>
<td align="left"><code>{n}</code></td>
<td align="left">바로 앞의 문자가 n번 이상 반복 (ERE)</td>
<td align="left"><code>a\{2,\}</code></td>
<td align="left"><code>a{2}</code> (aa, aaa...)</td>
</tr>
<tr>
<td align="left"><code>{n,m}</code></td>
<td align="left">바로 앞의 문자가 n번 이상 m번 이하 반복 (ERE)</td>
<td align="left"><code>a\{2,4\}</code></td>
<td align="left"><code>a{2,4}</code> (aa, aaa, aaaa)</td>
</tr>
<tr>
<td align="left"><code>\b</code></td>
<td align="left">단어 경계 (word boundary). 단어의 시작이나 끝을 의미 (GNU grep)</td>
<td align="left"><code>\bword\b</code></td>
<td align="left"><code>\bword\b</code></td>
</tr>
<tr>
<td align="left"><code>\w</code></td>
<td align="left">단어 문자 (알파벳, 숫자, 밑줄). <code>[A-Za-z0-9_]</code>와 유사 (GNU grep)</td>
<td align="left"><code>\w+</code></td>
<td align="left"><code>\w+</code></td>
</tr>
<tr>
<td align="left"><code>\s</code></td>
<td align="left">공백 문자 (스페이스, 탭 등) (GNU grep)</td>
<td align="left"><code>\s*</code></td>
<td align="left"><code>\s*</code></td>
</tr>
</tbody></table>
<h3>실제 활용 예시 확장</h3>
<ol>
<li><p><strong>로그 파일에서 특정 날짜의 에러 메시지만 추출:</strong></p>
<pre><code class="language-bash"># 2025년 7월 20일에 발생한 ERROR 메시지 검색
grep &quot;2025-07-20&quot; /var/log/syslog | grep &quot;ERROR&quot;
# 또는 정규표현식을 사용하여 한 번에 검색 (ERE 사용)
grep -E &quot;^2025-07-20.*ERROR&quot; /var/log/syslog
</code></pre>
</li>
<li><p><strong>특정 사용자(예: &#39;nginx&#39;)의 프로세스 찾고 CPU/메모리 사용량 확인:</strong></p>
<pre><code class="language-bash">ps aux | grep &quot;^nginx&quot; # nginx로 시작하는 사용자명의 프로세스
# ps aux의 출력에서 첫 번째 필드가 &#39;nginx&#39;인 라인만 grep
ps aux | grep -E &quot;^nginx\s+&quot;
</code></pre>
</li>
<li><p><strong>설정 파일에서 주석(#으로 시작) 및 빈 라인을 제외한 실제 설정값만 보기:</strong></p>
<pre><code class="language-bash">grep -v -E &quot;^\s*#|^\s*$&quot; /etc/ssh/sshd_config
# ^\s*# : 공백으로 시작할 수도 있는 주석 라인
# ^\s*$ : 공백만 있거나 아무것도 없는 빈 라인
# | : OR 조건
# -v : 위 패턴들을 제외
</code></pre>
</li>
<li><p><strong>소스 코드에서 함수 정의 찾기 (예: &#39;function_name(&#39; 패턴):</strong></p>
<pre><code class="language-bash">grep -E -n -r &quot;my_function\s*\(&quot; ./src_directory/
# -n: 라인 번호 표시, -r: 하위 디렉토리 검색
# \s*\( : 함수 이름 뒤에 공백이 있거나 없을 수 있고, 여는 괄호가 오는 패턴
</code></pre>
</li>
<li><p><strong>IP 주소만 추출하기:</strong></p>
<pre><code class="language-bash">grep -E -o &quot;([0-9]{1,3}\.){3}[0-9]{1,3}&quot; access.log
# -o: 일치하는 부분만 출력
# ([0-9]{1,3}\.){3} : 1~3자리 숫자와 점(.)이 3번 반복 (예: 192.168.1.)
# [0-9]{1,3} : 마지막 1~3자리 숫자 (예: 100)
</code></pre>
</li>
<li><p><strong>특정 확장자를 가진 파일에서만 검색:</strong></p>
<pre><code class="language-bash">grep &quot;search_term&quot; --include=\*.{c,h} -r ./project_dir/
# --include를 사용하여 .c 또는 .h 파일만 검색
grep &quot;api_key&quot; --exclude-dir=node_modules -r .
# --exclude-dir을 사용하여 특정 디렉토리(node_modules)를 검색에서 제외
</code></pre>
</li>
</ol>
<h2>AWK 명령어 심층 분석</h2>
<p><code>awk</code>는 패턴 검색과 텍스트/데이터 조작을 위한 강력한 프로그래밍 언어입니다. 주로 열(column) 기반의 데이터를 처리하고 보고서를 생성하는 데 탁월한 능력을 보입니다. C언어와 유사한 문법 구조를 가지며, 복잡한 조건 처리, 연산, 문자열 함수 등을 지원합니다.</p>
<h3>AWK 프로그램의 기본 구조</h3>
<pre><code class="language-awk">pattern { action }
</code></pre>
<ul>
<li><strong>pattern</strong>: 어떤 라인에 대해 <code>action</code>을 수행할지 결정하는 조건입니다. 생략되면 모든 라인에 대해 <code>action</code>이 수행됩니다.<ul>
<li>정규표현식: <code>/regex/</code></li>
<li>비교 표현식: <code>$1 == &quot;ERROR&quot;</code>, <code>$3 &gt; 100</code></li>
<li>문자열 매칭: <code>$2 ~ &quot;pattern&quot;</code> (매칭), <code>$2 !~ &quot;pattern&quot;</code> (비매칭)</li>
<li>특별한 패턴: <code>BEGIN</code>, <code>END</code></li>
</ul>
</li>
<li><strong>action</strong>: 중괄호 <code>{}</code> 안에 기술되며, 세미콜론(;)으로 구분된 하나 이상의 <code>awk</code> 명령어들로 구성됩니다. 생략되면 기본 동작은 <code>print $0</code> (현재 라인 전체 출력)입니다.</li>
</ul>
<p><strong>특별 패턴:</strong></p>
<ul>
<li><code>BEGIN { actions }</code>: 입력 파일을 읽기 전에 한 번 실행됩니다. 변수 초기화, 헤더 출력 등에 사용됩니다.</li>
<li><code>END { actions }</code>: 모든 입력 파일 처리가 끝난 후 한 번 실행됩니다. 총계 계산, 요약 보고서 출력 등에 사용됩니다.</li>
</ul>
<h3>필드 처리와 구분자</h3>
<p><code>awk</code>는 입력 라인을 **필드(field)**로 자동 분리합니다.</p>
<ul>
<li><code>$0</code>: 현재 처리 중인 라인 전체를 나타냅니다.</li>
<li><code>$1</code>, <code>$2</code>, <code>$3</code>, ...: 첫 번째, 두 번째, 세 번째 필드를 나타냅니다.</li>
<li><strong>필드 구분자 (Field Separator)</strong>:<ul>
<li><code>FS</code> (Input Field Separator): 입력 필드 구분자입니다. 기본값은 공백(스페이스, 탭)입니다.<ul>
<li>명령줄에서 <code>-F</code> 옵션으로 지정: <code>awk -F, &#39;{print $1}&#39; data.csv</code> (쉼표로 구분된 CSV 파일)</li>
<li><code>BEGIN</code> 블록에서 설정: <code>awk &#39;BEGIN { FS = &quot;:&quot; } {print $1, $7}&#39; /etc/passwd</code> (콜론으로 구분된 파일)</li>
</ul>
</li>
<li><code>OFS</code> (Output Field Separator): 출력 필드 구분자입니다. 기본값은 공백입니다.<ul>
<li><code>BEGIN</code> 블록에서 설정: <code>awk &#39;BEGIN { OFS = &quot;\t&quot; } {print $1, $2}&#39; file.txt</code> (탭으로 구분하여 출력)</li>
</ul>
</li>
</ul>
</li>
</ul>
<!-- end list -->

<pre><code class="language-bash"># /etc/passwd 파일에서 사용자 이름(첫 번째 필드)과 셸(일곱 번째 필드) 출력, 콜론으로 구분
awk -F: &#39;{print &quot;User:&quot;, $1, &quot;Shell:&quot;, $7}&#39; /etc/passwd

# CSV 파일에서 첫 번째와 세 번째 필드를 탭으로 구분하여 출력
awk -F, &#39;BEGIN{OFS=&quot;\t&quot;} {print $1, $3}&#39; data.csv
</code></pre>
<h3>조건문과 패턴 상세</h3>
<p><code>awk</code>는 다양한 조건과 패턴을 사용하여 특정 라인에 대해서만 액션을 수행할 수 있습니다.</p>
<ul>
<li><p><strong>정규표현식 패턴</strong>:</p>
<pre><code class="language-awk"># &#39;error&#39; 또는 &#39;fail&#39; (대소문자 무시) 문자열이 포함된 라인의 전체 내용 출력
/[Ee]rror|[Ff]ail/ {print $0}
</code></pre>
<p><em>(쉘에서 실행 시)</em></p>
<pre><code class="language-bash">awk &#39;/[Ee]rror|[Ff]ail/ {print $0}&#39; log.txt
</code></pre>
</li>
<li><p><strong>비교 표현식 패턴</strong>:</p>
<pre><code class="language-awk"># 세 번째 필드의 값이 100보다 큰 라인만 출력
$3 &gt; 100 {print &quot;Large Value Found:&quot;, $0}
</code></pre>
<p><em>(쉘에서 실행 시)</em></p>
<pre><code class="language-bash">awk &#39;$3 &gt; 100 {print &quot;Large Value Found:&quot;, $0}&#39; data.txt
</code></pre>
<pre><code class="language-awk"># 첫 번째 필드가 정확히 &quot;user_admin&quot;인 라인의 두 번째와 네 번째 필드 출력
$1 == &quot;user_admin&quot; {print $2, $4}
</code></pre>
<p><em>(쉘에서 실행 시)</em></p>
<pre><code class="language-bash">awk &#39;$1 == &quot;user_admin&quot; {print $2, $4}&#39; users.txt
</code></pre>
</li>
<li><p><strong>범위 패턴</strong>: 특정 시작 패턴부터 특정 끝 패턴까지의 모든 라인에 대해 액션을 수행합니다.</p>
<pre><code class="language-awk"># &quot;START_SECTION&quot; 라인부터 &quot;END_SECTION&quot; 라인까지 출력
/START_SECTION/,/END_SECTION/ {print}
</code></pre>
<p><em>(쉘에서 실행 시)</em></p>
<pre><code class="language-bash">awk &#39;/START_SECTION/,/END_SECTION/ {print}&#39; config.file
</code></pre>
</li>
<li><p><strong>논리 연산자</strong>: <code>&amp;&amp;</code> (AND), <code>||</code> (OR), <code>!</code> (NOT)을 사용하여 복합적인 조건을 만들 수 있습니다.</p>
<pre><code class="language-awk"># 첫 번째 필드가 &quot;admin&quot;이고, 네 번째 필드가 0보다 큰 라인 출력
$1 == &quot;admin&quot; &amp;&amp; $4 &gt; 0 {print $0}
</code></pre>
<p><em>(쉘에서 실행 시)</em></p>
<pre><code class="language-bash">awk &#39;$1 == &quot;admin&quot; &amp;&amp; $4 &gt; 0 {print $0}&#39; access_records.txt
</code></pre>
</li>
<li><p><strong>액션 내의 조건문 (<code>if-else</code>)</strong>:</p>
<pre><code class="language-awk">{
    if ($3 &gt; 1000) {
        status = &quot;High&quot;
    } else if ($3 &gt; 500) {
        status = &quot;Medium&quot;
    } else {
        status = &quot;Low&quot;
    }
    print $1, $2, $3, status
}
</code></pre>
<p><em>(쉘에서 실행 시)</em></p>
<pre><code class="language-bash">awk &#39;{
    if ($3 &gt; 1000) {
        status = &quot;High&quot;
    } else if ($3 &gt; 500) {
        status = &quot;Medium&quot;
    } else {
        status = &quot;Low&quot;
    }
    print $1, $2, $3, status
}&#39; resource_usage.txt
</code></pre>
</li>
</ul>
<h3>주요 내장 변수</h3>
<p><code>awk</code>는 다양한 내장 변수를 제공하여 프로그래밍을 용이하게 합니다.</p>
<table>
<thead>
<tr>
<th align="left">변수명</th>
<th align="left">설명</th>
<th align="left">예시 (쉘에서 실행 시)</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>NR</code></td>
<td align="left">Number of Records. 현재까지 처리한 총 레코드(라인)의 수.</td>
<td align="left"><code>awk &#39;{print NR, $0}&#39; file.txt</code> (라인 번호와 함께 출력)</td>
</tr>
<tr>
<td align="left"><code>FNR</code></td>
<td align="left">File Number of Record. 현재 처리 중인 파일 내에서의 레코드(라인) 수. 다중 파일 처리 시 유용.</td>
<td align="left"><code>awk &#39;{print FILENAME, FNR, $0}&#39; file1.txt file2.txt</code></td>
</tr>
<tr>
<td align="left"><code>NF</code></td>
<td align="left">Number of Fields. 현재 레코드(라인)의 필드 개수.</td>
<td align="left"><code>awk &#39;{print &quot;Line&quot;, NR, &quot;has&quot;, NF, &quot;fields:&quot;, $0}&#39; file.txt</code></td>
</tr>
<tr>
<td align="left"><code>FILENAME</code></td>
<td align="left">현재 처리 중인 입력 파일의 이름.</td>
<td align="left"><code>awk &#39;/error/ {print FILENAME, &quot;:&quot;, $0}&#39; *.log</code></td>
</tr>
<tr>
<td align="left"><code>FS</code></td>
<td align="left">Input Field Separator. 입력 필드 구분자 (기본값: 공백).</td>
<td align="left"><code>awk -F&#39;;&#39; &#39;{print $1}&#39; data.csv</code> &lt;br&gt; <code>awk &#39;BEGIN{FS=&quot;;&quot;} {print $1}&#39; data.csv</code></td>
</tr>
<tr>
<td align="left"><code>OFS</code></td>
<td align="left">Output Field Separator. 출력 필드 구분자 (기본값: 공백).</td>
<td align="left"><code>awk &#39;BEGIN{OFS=&quot;,&quot;} {print $1, $2}&#39; file.txt</code> (쉼표로 구분하여 출력)</td>
</tr>
<tr>
<td align="left"><code>RS</code></td>
<td align="left">Record Separator. 입력 레코드 구분자 (기본값: 개행 문자 <code>\n</code>).</td>
<td align="left"><code>awk &#39;BEGIN{RS=&quot;\n\n&quot;} {print $1}&#39; paragraphs.txt</code> (빈 줄로 구분된 단락 처리)</td>
</tr>
<tr>
<td align="left"><code>ORS</code></td>
<td align="left">Output Record Separator. 출력 레코드 구분자 (기본값: 개행 문자 <code>\n</code>).</td>
<td align="left"><code>awk &#39;BEGIN{ORS=&quot;;&quot;} {print $1}&#39; items.txt</code> (세미콜론으로 레코드 구분 출력)</td>
</tr>
<tr>
<td align="left"><code>ARGC</code></td>
<td align="left">Argument Count. 명령줄 인수의 개수.</td>
<td align="left"><code>awk &#39;BEGIN{print &quot;ARGC:&quot;, ARGC}&#39; file.txt</code></td>
</tr>
<tr>
<td align="left"><code>ARGV</code></td>
<td align="left">Argument Vector. 명령줄 인수를 담고 있는 배열. <code>ARGV[0]</code>은 <code>awk</code> 자체.</td>
<td align="left"><code>awk &#39;BEGIN{for(i=0;i&lt;ARGC;i++) print ARGV[i]}&#39; file.txt</code></td>
</tr>
<tr>
<td align="left"><code>ENVIRON</code></td>
<td align="left">환경 변수에 접근할 수 있는 연관 배열.</td>
<td align="left"><code>awk &#39;BEGIN{print &quot;User:&quot;, ENVIRON[&quot;USER&quot;]}&#39;</code></td>
</tr>
</tbody></table>
<h3>AWK 스크립팅: 변수, 배열, 루프</h3>
<p><code>awk</code>는 단순한 필터링을 넘어 스크립팅 언어로서의 기능도 제공합니다.</p>
<ul>
<li><p><strong>변수 사용</strong>:</p>
<pre><code class="language-awk"># 세 번째 필드의 합계 계산
{ sum += $3 } END { print &quot;Total Sum:&quot;, sum }
</code></pre>
<p><em>(쉘에서 실행 시)</em></p>
<pre><code class="language-bash">awk &#39;{ sum += $3 } END { print &quot;Total Sum:&quot;, sum }&#39; numbers.txt
</code></pre>
</li>
<li><p><strong>연관 배열 (Associative Arrays)</strong>: <code>awk</code>의 배열은 숫자 인덱스뿐만 아니라 문자열 인덱스도 사용할 수 있습니다.</p>
<pre><code class="language-awk"># 첫 번째 필드(예: IP 주소)의 등장 횟수 계산
{ counts[$1]++ } END { for (ip in counts) print ip, &quot;occurred&quot;, counts[ip], &quot;times&quot; }
</code></pre>
<p><em>(쉘에서 실행 시)</em></p>
<pre><code class="language-bash">awk &#39;{ counts[$1]++ } END { for (ip in counts) print ip, &quot;occurred&quot;, counts[ip], &quot;times&quot; }&#39; access.log
</code></pre>
</li>
<li><p><strong>루프 (Loops)</strong>: <code>for</code> 루프와 <code>while</code> 루프를 사용할 수 있습니다.</p>
<pre><code class="language-awk"># 각 라인의 모든 필드를 역순으로 출력
{
    for (i = NF; i &gt;= 1; i--) {
        printf &quot;%s &quot;, $i
    }
    printf &quot;\n&quot; # 또는 print &quot;&quot;
}
</code></pre>
<p><em>(쉘에서 실행 시)</em></p>
<pre><code class="language-bash">awk &#39;{
    for (i = NF; i &gt;= 1; i--) {
        printf &quot;%s &quot;, $i
    }
    printf &quot;\n&quot; # 또는 print &quot;&quot;
}&#39; data.txt
</code></pre>
<pre><code class="language-awk"># 1부터 5까지 숫자 출력 (BEGIN 블록에서)
BEGIN { i=1; while(i&lt;=5) { print i; i++ } }
</code></pre>
<p><em>(쉘에서 실행 시)</em></p>
<pre><code class="language-bash">awk &#39;BEGIN { i=1; while(i&lt;=5) { print i; i++ } }&#39;
</code></pre>
</li>
<li><p><strong>사용자 정의 함수 (User-Defined Functions)</strong>:</p>
<pre><code class="language-awk">function format_size(bytes,    suffix_idx, suffixes) { # 로컬 변수는 추가 파라미터로 선언
    suffixes = &quot;Bytes KB MB GB TB PB&quot;
    split(suffixes, suffix_array, &quot; &quot;)
    suffix_idx = 1
    while (bytes &gt;= 1024 &amp;&amp; suffix_idx &lt; length(suffix_array)) {
        bytes /= 1024
        suffix_idx++
    }
    return sprintf(&quot;%.2f %s&quot;, bytes, suffix_array[suffix_idx])
}

# 파일 크기(ls -l의 5번째 필드)를 읽기 쉽게 포맷팅 (NR &gt; 1은 헤더 제외)
# 쉘에서 실행하는 예시
# ls -l | awk &#39;
# function format_size(bytes, suffix_idx, suffixes) {
#     suffixes = &quot;Bytes KB MB GB TB PB&quot;; split(suffixes, suffix_array, &quot; &quot;); suffix_idx = 1
#     while (bytes &gt;= 1024 &amp;&amp; suffix_idx &lt; length(suffix_array)) { bytes /= 1024; suffix_idx++ }
#     return sprintf(&quot;%.2f %s&quot;, bytes, suffix_array[suffix_idx])
# }
# NR &gt; 1 {print $9, format_size($5)}&#39;
</code></pre>
<p><em>(위 <code>ls -l</code> 예시를 쉘에서 바로 실행할 수 있는 형태로 아래에 제공)</em></p>
<pre><code class="language-bash">ls -l | awk &#39;
function format_size(bytes,    suffix_idx, suffixes) {
    # 로컬 변수는 추가 파라미터로 선언하는 것이 좋은 습관입니다.
    # suffixes와 suffix_idx를 파라미터로 선언하여 로컬 변수처럼 사용합니다.
    # 실제 값은 함수 호출 시 전달하지 않아도 됩니다.
    suffixes = &quot;Bytes KB MB GB TB PB&quot;
    split(suffixes, suffix_array, &quot; &quot;)
    suffix_idx = 1
    while (bytes &gt;= 1024 &amp;&amp; suffix_idx &lt; length(suffix_array)) {
        bytes /= 1024
        suffix_idx++
    }
    return sprintf(&quot;%.2f %s&quot;, bytes, suffix_array[suffix_idx])
}
NR &gt; 1 {print $9, format_size($5)}&#39;
</code></pre>
</li>
</ul>
<h2>실무 활용 예시 심층 분석</h2>
<h3>시스템 모니터링</h3>
<ol>
<li><p><strong>메모리 사용량이 특정 임계값(예: 5.0%)을 초과하는 프로세스 찾기 (프로세스 ID, 메모리 사용률, 명령어 출력):</strong></p>
<pre><code class="language-bash">ps aux | awk &#39;NR &gt; 1 &amp;&amp; $4 &gt; 5.0 {printf &quot;PID: %s, %%MEM: %s, CMD: %s\n&quot;, $2, $4, $11}&#39;
# NR &gt; 1 : 헤더 라인 제외
# $4 &gt; 5.0 : 4번째 필드(메모리 사용률)가 5.0보다 큰 경우
# printf : 형식화된 출력
</code></pre>
</li>
<li><p><strong>디스크 사용량이 90% 이상인 파티션 찾기 (파일 시스템, 사용률, 마운트 지점 출력):</strong></p>
<pre><code class="language-bash">df -h | awk &#39;NR &gt; 1 &amp;&amp; int($5) &gt; 90 {print &quot;FS:&quot;, $1, &quot;| Usage:&quot;, $5, &quot;| Mounted on:&quot;, $6}&#39;
# int($5) : % 기호를 제거하고 정수형으로 변환 (GNU awk에서는 $5+0 으로도 가능)
# df -P | awk &#39;NR &gt; 1 &amp;&amp; $5+0 &gt; 90 {print $1, $5, $6}&#39; # POSIX 호환, % 자동 제거
</code></pre>
</li>
<li><p><strong>특정 포트(예: 80)를 사용하는 네트워크 연결 상태 보기:</strong></p>
<pre><code class="language-bash">netstat -tulnp | grep &quot;:80\s&quot; | awk &#39;{print $4, $5, $7}&#39;
# $4: Local Address, $5: Foreign Address, $7: PID/Program name
</code></pre>
</li>
</ol>
<h3>로그 분석</h3>
<ol>
<li><p><strong>웹 서버 로그(common log format)에서 특정 시간대(예: 10시부터 11시 사이)의 404 에러 로그 추출 및 요청 경로 출력:</strong></p>
<pre><code class="language-bash"># 예: 20/Jul/2025:10:00:00 부터 20/Jul/2025:10:59:59 까지
grep &quot;20/Jul/2025:10:&quot; access.log | awk &#39;$9 == &quot;404&quot; {print $1, $4, $7, $9}&#39;
# $1: IP, $4: Timestamp, $7: Requested Path, $9: Status Code
</code></pre>
</li>
<li><p><strong>IP 주소별 접속 횟수 계산 및 상위 10개 IP 출력:</strong></p>
<pre><code class="language-bash">awk &#39;{print $1}&#39; access.log | sort | uniq -c | sort -nr | head -n 10
# awk &#39;{print $1}&#39; : 첫 번째 필드(IP 주소)만 추출
# sort : IP 주소 정렬
# uniq -c : 중복 제거 및 횟수 카운트
# sort -nr : 숫자 기준 내림차순 정렬 (가장 많은 접속부터)
# head -n 10 : 상위 10개만 출력
</code></pre>
<p><em>AWK 만으로도 가능:</em></p>
<pre><code class="language-bash">awk &#39;{ip_counts[$1]++} END {for (ip in ip_counts) print ip_counts[ip], ip}&#39; access.log | sort -nr | head -n 10
</code></pre>
</li>
<li><p><strong>SSH 로그인 실패 로그에서 시도된 사용자 이름과 IP 주소 추출:</strong></p>
<pre><code class="language-bash">grep &quot;Failed password&quot; /var/log/auth.log | awk &#39;{print &quot;User:&quot;, $(NF-5), &quot;IP:&quot;, $(NF-3)}&#39;
# &quot;Failed password for invalid user admin from 1.2.3.4 port 12345 ssh2&quot; 같은 로그 형식 가정
# $(NF-5) : 뒤에서 6번째 필드 (사용자 이름)
# $(NF-3) : 뒤에서 4번째 필드 (IP 주소)
# 로그 형식에 따라 필드 번호는 달라질 수 있음.
# 예시: &quot;Failed password for root from 116.37.102.13 port 40940 ssh2&quot;
# 이 경우 awk &#39;{print &quot;User:&quot;, $9, &quot;IP:&quot;, $11}&#39;
</code></pre>
</li>
</ol>
<h3>데이터 처리 및 변환</h3>
<ol>
<li><p><strong>CSV 파일에서 특정 조건(예: 3번째 열의 값이 &quot;Completed&quot;)을 만족하는 행의 1번째와 5번째 열을 새로운 CSV 파일로 저장:</strong></p>
<pre><code class="language-bash">awk -F, &#39;$3 == &quot;Completed&quot; {print $1 &quot;,&quot; $5}&#39; input.csv &gt; output.csv
</code></pre>
<p><em>또는 OFS 사용:</em></p>
<pre><code class="language-bash">awk -F, &#39;BEGIN{OFS=&quot;,&quot;} $3 == &quot;Completed&quot; {print $1, $5}&#39; input.csv &gt; output.csv
</code></pre>
</li>
<li><p><strong>텍스트 파일에서 모든 이메일 주소를 추출하여 한 줄에 하나씩 정렬 및 중복 제거:</strong></p>
<pre><code class="language-bash">grep -E -o &quot;\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,6}\b&quot; file.txt | sort | uniq
# grep -o : 일치하는 부분만 출력
# \b : 단어 경계
# [A-Za-z0-9._%+-]+ : 사용자 이름 부분
# @ : 구분자
# [A-Za-z0-9.-]+ : 도메인 이름 부분
# \.[A-Za-z]{2,6} : 최상위 도메인 부분 (예: .com, .co.kr)
</code></pre>
</li>
<li><p><strong>숫자 리스트 파일에서 각 숫자를 제곱하여 출력:</strong></p>
<pre><code># numbers.txt 내용 예시:
# 1
# 5
# 10
</code></pre>
<pre><code class="language-bash">awk &#39;{print $1, &quot;squared is&quot;, $1*$1}&#39; numbers.txt
</code></pre>
<p><em>출력:</em></p>
<pre><code># 1 squared is 1
# 5 squared is 25
# 10 squared is 100
</code></pre>
</li>
<li><p><strong>여러 파일의 특정 열 합계 계산 (예: 각 파일의 2번째 열 합계 및 총합):</strong></p>
<pre><code># data1.txt:
# itemA 10
# itemB 20
# data2.txt:
# itemC 30
# itemD 40
</code></pre>
<p><em>GNU Awk 확장 사용 시:</em></p>
<pre><code class="language-bash">awk &#39;
    { current_sum += $2 }
    ENDFILE { # GNU Awk 확장. 각 파일 처리 후 실행
        print &quot;Sum for&quot;, FILENAME, &quot;:&quot;, current_sum
        total_sum += current_sum
        current_sum = 0 # 다음 파일을 위해 초기화
    }
    END {
        print &quot;Grand Total:&quot;, total_sum
    }
&#39; data*.txt
</code></pre>
<p><em>POSIX 호환 (FNR 사용):</em></p>
<pre><code class="language-bash">awk &#39;
    FNR == 1 &amp;&amp; NR != 1 { # 새 파일 시작 (첫 파일 제외)
        print &quot;Sum for&quot;, prev_filename, &quot;:&quot;, current_sum
        total_sum += current_sum
        current_sum = 0
    }
    { current_sum += $2; prev_filename = FILENAME }
    END {
        print &quot;Sum for&quot;, prev_filename, &quot;:&quot;, current_sum # 마지막 파일 합계
        total_sum += current_sum
        print &quot;Grand Total:&quot;, total_sum
    }
&#39; data*.txt
</code></pre>
</li>
</ol>
<h2>Grep과 AWK의 조합</h2>
<p><code>grep</code>으로 1차 필터링을 하고, 그 결과를 파이프(<code>|</code>)를 통해 <code>awk</code>로 넘겨 2차 가공하는 방식은 매우 효과적입니다.</p>
<pre><code class="language-bash"># /var/log/secure 에서 &quot;Accepted publickey&quot; 로그 중 사용자명과 IP 주소만 추출
grep &quot;Accepted publickey&quot; /var/log/secure | awk &#39;{print &quot;User:&quot;, $9, &quot;IP:&quot;, $11, &quot;Port:&quot;, $13}&#39;

# 현재 디렉토리의 C 소스 파일(*.c)들 중에서 &quot;include&quot; 라는 단어를 포함하는 라인을 찾고,
# 그 라인들에서 파일명과 라인번호, 그리고 #include 뒤의 헤더 파일 이름만 추출
grep -Hn &quot;include&quot; *.c | awk -F&#39;[:&lt;&gt;]&#39; &#39;{print &quot;File: &quot; $1 &quot;, Line: &quot; $2 &quot;, Header: &quot; $4}&#39;
# grep -H: 파일명 출력, -n: 라인번호 출력
# awk -F&#39;[:&lt;&gt;]&#39;: 콜론(:), 여는 꺽쇠(&lt;), 닫는 꺽쇠(&gt;)를 구분자로 사용
# 예: main.c:10:#include &lt;stdio.h&gt; -&gt; $1=main.c, $2=10, $3=#include , $4=stdio.h
</code></pre>
<h2>결론</h2>
<p><code>grep</code>과 <code>awk</code>는 리눅스/유닉스 환경에서 텍스트를 다루는 데 있어 스위스 군용 칼과 같은 존재입니다. <code>grep</code>은 강력한 패턴 검색 능력으로 원하는 정보를 빠르게 찾아내고, <code>awk</code>는 구조화된 데이터 처리와 리포팅에 특화되어 있습니다. 이 두 도구의 다양한 옵션과 정규표현식, 그리고 <code>awk</code>의 프로그래밍 기능을 깊이 이해하고 조합하여 사용한다면, 복잡하고 방대한 텍스트 데이터도 손쉽게 원하는 형태로 가공하고 분석할 수 있습니다.</p>
<p>물론 매우 복잡한 로직이나 대규모 데이터 처리에는 Python, Perl과 같은 범용 스크립팅 언어가 더 적합할 수 있지만, 대부분의 일상적인 시스템 관리, 로그 분석, 간단한 데이터 변환 작업은 <code>grep</code>과 <code>awk</code>만으로도 충분히 효율적으로 처리할 수 있습니다. 꾸준한 연습과 다양한 실제 문제에 적용해보는 것이 이 도구들을 마스터하는 가장 좋은 방법입니다.</p>

        </div>
    </article>
</body>
</html>